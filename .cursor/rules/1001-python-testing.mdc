---
description: ENFORCE comprehensive Python testing standards for reliable and maintainable test suites
globs: ["**/tests/**/*.py", "**/test_*.py", "**/*_test.py"]
crossRefs:
  - 001-rule-generator.mdc
  - 002-rule-organization.mdc
  - 1000-python-code-style.mdc
alwaysApply: true
---

# Python Testing Standards

## Context
- When writing unit tests
- When implementing integration tests
- When setting up test infrastructure
- When reviewing test code
- When maintaining test suites

## Requirements

### Test Structure
- Follow pytest conventions:
  ```python
  # Good: Well-organized test module
  """Tests for data processing module."""

  import pytest
  import pandas as pd
  from src.data import process_data

  class TestDataProcessing:
      """Test suite for data processing functions."""

      @pytest.fixture
      def sample_data(self) -> pd.DataFrame:
          """Create sample data for testing."""
          return pd.DataFrame({
              "col1": [1, 2, 3],
              "col2": [4, 5, 6],
          })

      def test_process_data_basic(self, sample_data: pd.DataFrame) -> None:
          """Test basic data processing functionality."""
          result = process_data(sample_data, ["col1"])
          assert isinstance(result, pd.DataFrame)
          assert "col1" in result.columns
  ```

### Test Coverage
- Maintain minimum 80% test coverage
- Use pytest-cov for coverage reporting:
  ```python
  # pytest.ini
  [pytest]
  addopts = --cov=src --cov-report=term-missing
  ```

### Test Categories
- Unit tests for individual components
- Integration tests for component interaction
- Property-based tests for data transformations
- Performance tests for critical paths

### Test Data Management
- Use fixtures for reusable test data:
  ```python
  # Good: Comprehensive test fixtures
  @pytest.fixture
  def model_config() -> dict[str, float]:
      """Create standard model configuration."""
      return {
          "learning_rate": 0.01,
          "max_depth": 5,
          "n_estimators": 100,
      }

  @pytest.fixture
  def sample_dataset() -> tuple[np.ndarray, np.ndarray]:
      """Create sample dataset for model testing."""
      X = np.random.randn(100, 5)
      y = np.random.randint(0, 2, 100)
      return X, y
  ```

### Test Documentation
- Document test purpose and setup:
  ```python
  # Good: Well-documented test
  def test_model_training(
      model: BaseModel,
      sample_dataset: tuple[np.ndarray, np.ndarray],
  ) -> None:
      """Test model training process.

      This test verifies that:
      1. Model can be trained on sample data
      2. Training metrics are within expected ranges
      3. Model state is properly updated
      """
      X, y = sample_dataset
      model.fit(X, y)
      assert model.is_fitted()
      assert model.score(X, y) > 0.8
  ```

## Examples

<example>
# Good: Complete test suite structure
"""Test suite for data processing pipeline."""

import pytest
import pandas as pd
import numpy as np
from typing import Generator

from src.data import DataProcessor
from src.models import ModelConfig

@pytest.fixture
def processor() -> DataProcessor:
    """Create data processor instance."""
    return DataProcessor(config={"threshold": 0.5})

@pytest.fixture
def sample_data() -> Generator[pd.DataFrame, None, None]:
    """Generate sample data for testing."""
    data = pd.DataFrame({
        "feature1": np.random.randn(100),
        "feature2": np.random.randn(100),
        "target": np.random.randint(0, 2, 100),
    })
    yield data
    # Cleanup if needed

class TestDataProcessor:
    """Test suite for DataProcessor class."""

    def test_initialization(self, processor: DataProcessor) -> None:
        """Test processor initialization."""
        assert processor.config["threshold"] == 0.5

    def test_data_validation(
        self,
        processor: DataProcessor,
        sample_data: pd.DataFrame,
    ) -> None:
        """Test data validation functionality."""
        # Test valid data
        processor.process(sample_data, ["feature1", "feature2"])

        # Test invalid data
        with pytest.raises(ValueError):
            processor.process(pd.DataFrame(), ["feature1"])

    @pytest.mark.parametrize("threshold", [0.0, 0.5, 1.0])
    def test_threshold_processing(
        self,
        processor: DataProcessor,
        sample_data: pd.DataFrame,
        threshold: float,
    ) -> None:
        """Test data processing with different thresholds."""
        processor.config["threshold"] = threshold
        result = processor.process(sample_data, ["feature1"])
        assert all(result["feature1"] >= threshold)
</example>

<example type="invalid">
# Bad: Poor test structure
import pytest
import pandas as pd

def test_something():
    # Test data
    data = pd.DataFrame({"col": [1, 2]})
    
    # Test processing
    result = process_data(data)
    
    # Assertions
    assert result is not None
    assert len(result) > 0
    # Missing proper structure
    # Missing docstrings
    # Missing type hints
    # Poor test organization
</example>

## Best Practices
1. Use pytest fixtures for test data
2. Maintain test isolation
3. Write descriptive test names
4. Document test requirements
5. Use appropriate assertions
6. Test edge cases
7. Implement proper cleanup
8. Use parameterized tests
9. Monitor test performance
10. Keep tests maintainable

## Technical Metadata
- Category: Python Testing
- Priority: High
- Dependencies:
  - pytest
  - pytest-cov
  - pytest-mock
  - hypothesis
- Validation Requirements:
  - Test coverage
  - Test isolation
  - Documentation
  - Performance benchmarks

<version>1.0.0</version> 